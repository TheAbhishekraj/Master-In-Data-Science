{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ans: Lasso is a modification of linear regression, where the model is penalized for the sum of absolute values of the weights. Thus, the absolute values of weight will be (in general) reduced, and many will tend to be zeros. \n",
    "\n",
    "Lasso Regression is different from ridge regression as it uses absolute coefficient values for normalization. As loss function only considers absolute coefficients (weights), the optimization algorithm will penalize high coefficients. This is known as the L1 norm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ans: \n",
    "\n",
    "**Advantages of LASSO regression**\n",
    "\n",
    "Automatic features selection. The main advantage of a LASSO regression model is that it has the ability to set the coefficients for features it does not consider interesting to zero. This means that the model does some automatic feature selection to decide which features should and should not be included on its own.\n",
    "\n",
    "Reduced overfitting. Another advantage of a LASSO regression is that the L1 penalty that is added to the model helps to prevent the model from overfitting. This makes intuitive sense because when the model sets feature coefficients to zero and effectively removes features from the model, model complexity decreases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: Non-Zero Coefficients: In Lasso regression, the regularization penalty can lead to some of the coefficients being shrunk to zero, resulting in a sparse model. The number of non-zero coefficients can be used to evaluate the effectiveness of the regularization and feature selection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ans: Lasso regression encourages sparsity in the model by shrinking some coefficients to exactly zero. This results in a model with fewer predictors, which is easier to interpret and can lead to better prediction accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ans : No. The ordinary lasso penalty has been extensively used in the framework of linear regression models; however, sufficient results have not been obtained for nonlinear regression models with Gaussian basis functions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "\n",
    "|Ridge Regression|\tLasso Regression|\n",
    "|----------------|------------------|\n",
    "|The penalty term is the sum of the squares of the coefficients (L2 regularization).|\tThe penalty term is the sum of the absolute values of the coefficients (L1 regularization).|\n",
    "|Shrinks the coefficients but doesn’t set any coefficient to zero.\t|Can shrink some coefficients to zero, effectively performing feature selection.|\n",
    "|Helps to reduce overfitting by shrinking large coefficients.\t|Helps to reduce overfitting by shrinking and selecting features with less importance.|\n",
    "|Works well when there are a large number of features.\t|Works well when there are a small number of features.|\n",
    "|Performs “soft thresholding” of coefficients.|\tPerforms “hard thresholding” of coefficients.|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ans: Tolerant Method for dealing with multicollinearity known as Least Absolute Shrinkage and Selection Operator (LASSO) regression, solves the same constrained optimization problem as ridge regression, but uses the L1 norm rather than the L2 norm as a measure of complexity.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ans: \n",
    "When choosing a lambda value, the goal is to strike the right balance between simplicity and training-data fit: If your lambda value is too high, your model will be simple, but you run the risk of underfitting your data. Your model won't learn enough about the training data to make useful predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
