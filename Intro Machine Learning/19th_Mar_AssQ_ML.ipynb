{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: \n",
    "\n",
    "Min-max normalization is one of the most common ways to normalize data. For every feature, the minimum value of that feature gets transformed into a 0, the maximum value gets transformed into a 1, and every other value gets transformed into a decimal between 0 and 1.\n",
    "\n",
    "For example, if the minimum value of a feature was 20, and the maximum value was 40, then 30 would be transformed to about 0.5 since it is halfway between 20 and 40. The formula is as follows:\n",
    "(max−min)/(value−min)\n",
    "​\n",
    "Min-max normalization has one fairly significant downside: it does not handle outliers very well. For example, if you have 99 values between 0 and 40, and one value is 100, then the 99 values will all be transformed to a value between 0 and 0.4. That data is just as squished as before! Take a look at the image below to see an example of this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.97596444 -1.61155897]\n",
      " [-0.66776515  0.08481889]\n",
      " [-1.28416374  1.10264561]\n",
      " [ 0.97596444  0.42409446]]\n"
     ]
    }
   ],
   "source": [
    "# import module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create data\n",
    "data = [[11, 2], [3, 7], [0, 10], [11, 8]]\n",
    "\n",
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(data)\n",
    "scaled_data = model.transform(data)\n",
    "\n",
    "# print scaled data\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: Unit Vector Scaling scales the features so that each data point has a length of 1. This technique is useful when the angle between the data points is essential.\n",
    "This scaler scales each feature to have a length of 1. The formula is as follows:\n",
    "z = x / ||x||\n",
    "where x is the feature vector, ||x|| is the Euclidean norm of the feature vector, and z is the scaled feature vector.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  \n",
       "0                          3.92   1065.0  \n",
       "1                          3.40   1050.0  \n",
       "2                          3.17   1185.0  \n",
       "3                          3.45   1480.0  \n",
       "4                          2.93    735.0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Ans:\n",
    "\n",
    "# Principal component analysis, or PCA, is a dimensionality reduction method that is often used to reduce the dimensionality of large data sets, \n",
    "# by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n",
    "\n",
    "#Dimensionality Reduction is simply reducing the number of features (columns) while retaining maximum information. Following are reasons for Dimensionality Reduction:\n",
    "\n",
    "# Dimensionality Reduction helps in data compression, and hence reduced storage space.\n",
    "# It reduces computation time.\n",
    "# It also helps remove redundant features, if any.\n",
    "# Removes Correlated Features.\n",
    "# Reducing the dimensions of data to 2D or 3D may allow us to plot and visualize it precisely. You can then observe patterns more clearly.\n",
    "# It is helpful in noise removal also and as a result of that, we can improve the performance of models.\n",
    "\n",
    "# Import librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Load the dataset\n",
    "data = load_wine()\n",
    "# Configuring pandas to show all features\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "# Converting data to a dataframe to view properly\n",
    "data1 = pd.DataFrame(data=data['data'],columns=data['feature_names'])\n",
    "df =data1\n",
    "# Printing first 5 observations\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.316751</td>\n",
       "      <td>-1.443463</td>\n",
       "      <td>-0.165739</td>\n",
       "      <td>-0.215631</td>\n",
       "      <td>0.693043</td>\n",
       "      <td>-0.223880</td>\n",
       "      <td>0.596427</td>\n",
       "      <td>0.065139</td>\n",
       "      <td>0.641443</td>\n",
       "      <td>1.020956</td>\n",
       "      <td>-0.451563</td>\n",
       "      <td>0.540810</td>\n",
       "      <td>-0.066239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.209465</td>\n",
       "      <td>0.333393</td>\n",
       "      <td>-2.026457</td>\n",
       "      <td>-0.291358</td>\n",
       "      <td>-0.257655</td>\n",
       "      <td>-0.927120</td>\n",
       "      <td>0.053776</td>\n",
       "      <td>1.024416</td>\n",
       "      <td>-0.308847</td>\n",
       "      <td>0.159701</td>\n",
       "      <td>-0.142657</td>\n",
       "      <td>0.388238</td>\n",
       "      <td>0.003637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.516740</td>\n",
       "      <td>-1.031151</td>\n",
       "      <td>0.982819</td>\n",
       "      <td>0.724902</td>\n",
       "      <td>-0.251033</td>\n",
       "      <td>0.549276</td>\n",
       "      <td>0.424205</td>\n",
       "      <td>-0.344216</td>\n",
       "      <td>-1.177834</td>\n",
       "      <td>0.113361</td>\n",
       "      <td>-0.286673</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.021717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.757066</td>\n",
       "      <td>-2.756372</td>\n",
       "      <td>-0.176192</td>\n",
       "      <td>0.567983</td>\n",
       "      <td>-0.311842</td>\n",
       "      <td>0.114431</td>\n",
       "      <td>-0.383337</td>\n",
       "      <td>0.643593</td>\n",
       "      <td>0.052544</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>0.759584</td>\n",
       "      <td>-0.242020</td>\n",
       "      <td>-0.369484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.008908</td>\n",
       "      <td>-0.869831</td>\n",
       "      <td>2.026688</td>\n",
       "      <td>-0.409766</td>\n",
       "      <td>0.298458</td>\n",
       "      <td>-0.406520</td>\n",
       "      <td>0.444074</td>\n",
       "      <td>0.416700</td>\n",
       "      <td>0.326819</td>\n",
       "      <td>-0.078366</td>\n",
       "      <td>-0.525945</td>\n",
       "      <td>-0.216664</td>\n",
       "      <td>-0.079364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  3.316751 -1.443463 -0.165739 -0.215631  0.693043 -0.223880  0.596427   \n",
       "1  2.209465  0.333393 -2.026457 -0.291358 -0.257655 -0.927120  0.053776   \n",
       "2  2.516740 -1.031151  0.982819  0.724902 -0.251033  0.549276  0.424205   \n",
       "3  3.757066 -2.756372 -0.176192  0.567983 -0.311842  0.114431 -0.383337   \n",
       "4  1.008908 -0.869831  2.026688 -0.409766  0.298458 -0.406520  0.444074   \n",
       "\n",
       "         7         8         9         10        11        12  \n",
       "0  0.065139  0.641443  1.020956 -0.451563  0.540810 -0.066239  \n",
       "1  1.024416 -0.308847  0.159701 -0.142657  0.388238  0.003637  \n",
       "2 -0.344216 -1.177834  0.113361 -0.286673  0.000584  0.021717  \n",
       "3  0.643593  0.052544  0.239413  0.759584 -0.242020 -0.369484  \n",
       "4  0.416700  0.326819 -0.078366 -0.525945 -0.216664 -0.079364  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalar = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scalar.fit_transform(df), columns=df.columns)\n",
    "df_scaled\n",
    "\n",
    "### Now we are ready to apply for PCA.\n",
    "pca = PCA()\n",
    "df_pca = pd.DataFrame(pca.fit_transform(df_scaled))\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 13)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Explained Varience')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### interesting to see how much variance each principal component captures.\n",
    "import matplotlib.pyplot as plt\n",
    "pd.DataFrame(pca.explained_variance_ratio_).plot.bar()\n",
    "plt.legend('')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Explained Varience')\n",
    "\n",
    "# Observataion :\n",
    "# The first 6 Principal Components are capturing around 80% of the variance so we can replace the 11 original features  with the new 5 features having 80% of the information. \n",
    "# So, we have reduced the 13 dimensions to only 6 dimensions while retaining most of the information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ans: Data compression: PCA can be used to reduce the dimensionality of high-dimensional datasets, making them easier to store and analyze.\n",
    "Feature extraction: PCA can be used to identify the most important features in a dataset, which can be used to build predictive models.\n",
    "\n",
    "**PCA Algorithm for Feature Extraction**\n",
    "\n",
    "The following represents 6 steps of principal component analysis (PCA) algorithm:\n",
    "\n",
    "1. Standardize the dataset: Standardizing / normalizing the dataset is the first step one would need to take before performing PCA. The PCA calculates a new projection of the given data set representing one or more features. The new axes are based on the standard deviation of the value of these features. So, a feature / variable with a high standard deviation will have a higher weight for the calculation of axis than a variable / feature with a low standard deviation. If the data is normalized / standardized, the standard deviation of all fetaures / variables get measured on the same scale. Thus, all variables have the same weight and PCA calculates relevant axis appropriately. Note that the data is standardized / normalized after creating training / test split. Python’s sklearn.preprocessing StandardScaler class can be used for standardizing the dataset.\n",
    "\n",
    "2. Construct the covariance matrix: Once the data is standardized, the next step is to create n X n-dimensional covariance matrix, where n is the number of dimensions in the dataset. The covariance matrix stores the pairwise covariances between the different features.  Note that a positive covariance between two features indicates that the features increase or decrease together, whereas a negative covariance indicates that the features vary in opposite directions. Python‘s Numpy cov method can be used to create covariance matrix.\n",
    "\n",
    "3. Perform Eigendecomposition of covariance matrix: The next step is to decompose the covariance matrix into its eigenvectors and eigenvalues. The eigenvectors of the covariance matrix represent the principal components (the directions of maximum variance), whereas the corresponding eigenvalues will define their magnitude. Numpy linalg.eig or linalg.eigh can be used for decomposing covariance matrix into eigenvectors and eigenvalues.\n",
    "\n",
    "4. Selection of most important Eigenvectors / Eigenvalues: Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors. Select k eigenvectors, which correspond to the k largest eigenvalues, where k is the dimensionality of the new feature subspace (). One can used the concepts of explained variance to select the k most important eigenvectors.\n",
    "\n",
    "5. Projection matrix creation of important eigenvectors: Construct a projection matrix, W, from the top k eigenvectors.\n",
    "\n",
    "6. Training / test dataset transformation: Finally, transform the d-dimensional input training and test dataset using the projection matrix to obtain the new k-dimensional feature subspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sl_no</th>\n",
       "      <th>gender</th>\n",
       "      <th>ssc_p</th>\n",
       "      <th>ssc_b</th>\n",
       "      <th>hsc_p</th>\n",
       "      <th>hsc_b</th>\n",
       "      <th>hsc_s</th>\n",
       "      <th>degree_p</th>\n",
       "      <th>degree_t</th>\n",
       "      <th>workex</th>\n",
       "      <th>etest_p</th>\n",
       "      <th>specialisation</th>\n",
       "      <th>mba_p</th>\n",
       "      <th>status</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>67.00</td>\n",
       "      <td>Others</td>\n",
       "      <td>91.00</td>\n",
       "      <td>Others</td>\n",
       "      <td>Commerce</td>\n",
       "      <td>58.00</td>\n",
       "      <td>Sci&amp;Tech</td>\n",
       "      <td>No</td>\n",
       "      <td>55.0</td>\n",
       "      <td>Mkt&amp;HR</td>\n",
       "      <td>58.80</td>\n",
       "      <td>Placed</td>\n",
       "      <td>270000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>79.33</td>\n",
       "      <td>Central</td>\n",
       "      <td>78.33</td>\n",
       "      <td>Others</td>\n",
       "      <td>Science</td>\n",
       "      <td>77.48</td>\n",
       "      <td>Sci&amp;Tech</td>\n",
       "      <td>Yes</td>\n",
       "      <td>86.5</td>\n",
       "      <td>Mkt&amp;Fin</td>\n",
       "      <td>66.28</td>\n",
       "      <td>Placed</td>\n",
       "      <td>200000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>65.00</td>\n",
       "      <td>Central</td>\n",
       "      <td>68.00</td>\n",
       "      <td>Central</td>\n",
       "      <td>Arts</td>\n",
       "      <td>64.00</td>\n",
       "      <td>Comm&amp;Mgmt</td>\n",
       "      <td>No</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Mkt&amp;Fin</td>\n",
       "      <td>57.80</td>\n",
       "      <td>Placed</td>\n",
       "      <td>250000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>56.00</td>\n",
       "      <td>Central</td>\n",
       "      <td>52.00</td>\n",
       "      <td>Central</td>\n",
       "      <td>Science</td>\n",
       "      <td>52.00</td>\n",
       "      <td>Sci&amp;Tech</td>\n",
       "      <td>No</td>\n",
       "      <td>66.0</td>\n",
       "      <td>Mkt&amp;HR</td>\n",
       "      <td>59.43</td>\n",
       "      <td>Not Placed</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>85.80</td>\n",
       "      <td>Central</td>\n",
       "      <td>73.60</td>\n",
       "      <td>Central</td>\n",
       "      <td>Commerce</td>\n",
       "      <td>73.30</td>\n",
       "      <td>Comm&amp;Mgmt</td>\n",
       "      <td>No</td>\n",
       "      <td>96.8</td>\n",
       "      <td>Mkt&amp;Fin</td>\n",
       "      <td>55.50</td>\n",
       "      <td>Placed</td>\n",
       "      <td>425000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sl_no gender  ssc_p    ssc_b  hsc_p    hsc_b     hsc_s  degree_p  \\\n",
       "0      1      M  67.00   Others  91.00   Others  Commerce     58.00   \n",
       "1      2      M  79.33  Central  78.33   Others   Science     77.48   \n",
       "2      3      M  65.00  Central  68.00  Central      Arts     64.00   \n",
       "3      4      M  56.00  Central  52.00  Central   Science     52.00   \n",
       "4      5      M  85.80  Central  73.60  Central  Commerce     73.30   \n",
       "\n",
       "    degree_t workex  etest_p specialisation  mba_p      status    salary  \n",
       "0   Sci&Tech     No     55.0         Mkt&HR  58.80      Placed  270000.0  \n",
       "1   Sci&Tech    Yes     86.5        Mkt&Fin  66.28      Placed  200000.0  \n",
       "2  Comm&Mgmt     No     75.0        Mkt&Fin  57.80      Placed  250000.0  \n",
       "3   Sci&Tech     No     66.0         Mkt&HR  59.43  Not Placed       NaN  \n",
       "4  Comm&Mgmt     No     96.8        Mkt&Fin  55.50      Placed  425000.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Import librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#load dataset\n",
    "\n",
    "data =\"Placement_Data_Full_Class.csv\"\n",
    "df = pd.read_csv(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#\n",
    "# Perform one-hot encoding\n",
    "#\n",
    "categorical_columns = df.columns[df.dtypes == object] # Find all categorical columns\n",
    " \n",
    "df = pd.get_dummies(df, columns = categorical_columns, drop_first=True)\n",
    "#\n",
    "# Create training / test split\n",
    "#\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = X_train, X_test, y_train, y_test = train_test_split(df[df.columns[df.columns != 'salary']],\n",
    "                   df['salary'], test_size=0.25, random_state=1)\n",
    "#\n",
    "# Standardize the dataset; This is very important before you apply PCA\n",
    "#\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "#\n",
    "# Import eigh method for calculating eigenvalues and eigenvectirs\n",
    "#\n",
    "from numpy.linalg import eigh\n",
    "#\n",
    "# Determine covariance matrix\n",
    "#\n",
    "cov_matrix = np.cov(X_train_std, rowvar=False)\n",
    "#\n",
    "# Determine eigenvalues and eigenvectors\n",
    "#\n",
    "egnvalues, egnvectors = eigh(cov_matrix)\n",
    "#\n",
    "# Determine explained variance and select the most important eigenvectors based on explained variance\n",
    "#\n",
    "total_egnvalues = sum(egnvalues)\n",
    "var_exp = [(i/total_egnvalues) for i in sorted(egnvalues, reverse=True)]\n",
    "#\n",
    "# Construct projection matrix using the five eigenvectors that correspond to the top five eigenvalues (largest), to capture about 75% of the variance in this dataset\n",
    "#\n",
    "egnpairs = [(np.abs(egnvalues[i]), egnvectors[:, i])\n",
    "                for i in range(len(egnvalues))]\n",
    "egnpairs.sort(key=lambda k: k[0], reverse=True)\n",
    "projectionMatrix = np.hstack((egnpairs[0][1][:, np.newaxis],\n",
    "                              egnpairs[1][1][:, np.newaxis],\n",
    "                              egnpairs[2][1][:, np.newaxis],\n",
    "                              egnpairs[3][1][:, np.newaxis],\n",
    "                              egnpairs[4][1][:, np.newaxis]))\n",
    "#\n",
    "# Transform the training data set\n",
    "#\n",
    "X_train_pca = X_train_std.dot(projectionMatrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Ans:\n",
    "\n",
    "# import module\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max = MinMaxScaler()\n",
    "# compute required values\n",
    "data = [[1, 5, 10, 15, 20]]\n",
    "min_max.fit_transform(data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
